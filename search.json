[
  {
    "objectID": "research/working_papers/activism/index.html",
    "href": "research/working_papers/activism/index.html",
    "title": "The Effects of Hedge Fund Activism",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/working_papers/activism/index.html#abstract",
    "href": "research/working_papers/activism/index.html#abstract",
    "title": "The Effects of Hedge Fund Activism",
    "section": "Abstract",
    "text": "Abstract\nIn this paper I explore the relationship between the rise of hedge fund activism and firm outcomes, using a study design that explicitly takes into account how activists pick their targets. Contrary to much prior work, I find no evidence that activism is associated with increased firm operating performance or significant long-term returns once comparing to firms based on their similarity to the targets. However, activism does increase firm payouts to shareholders and decreases investment, consistent with the argument of many critics of activism. I also find that firm-level employment declines significantly following a targeting event, and that the subset of firms that experience an increase in operating performance also engage in higher levels of tax avoidance. The deregulation of proxy access rules, wholesale de-staggering of corporate boards, and the rise in importance of proxy advisory firms who frequently recommend voting for activist proposals have made firms more susceptible to aggressive activism over the past three decades. The results in this paper, coupled with the rhetorical shift in focus from short-term profits to sustainable growth by large institutional investors, suggest a re-framing of the public debate over the benefits of shareholder activism."
  },
  {
    "objectID": "research/working_papers/activism/index.html#important-figure",
    "href": "research/working_papers/activism/index.html#important-figure",
    "title": "The Effects of Hedge Fund Activism",
    "section": "Important figure",
    "text": "Important figure\nFigure 11 reports the average long-term buy-and-hold abnormal returns for the targeted  rms in our sample with its 95% confidence interval (in blue), as well as the average of the propensity-score weighted comparison returns for each target (in red). Panel (a) presents the results for  rms that are taken over within the five years following the event or pseudo-event, and Panel (b) reports the results for firms that are not taken over during this period.\n\n\n\nFigure 1"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html",
    "href": "research/working_papers/antitakeover/index.html",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "",
    "text": "Paper\nSSRN\nCode"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html#abstract",
    "href": "research/working_papers/antitakeover/index.html#abstract",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Abstract",
    "text": "Abstract\nCorporate governance scholars have engaged in a longstanding debate over the impact of state antitakeover provisions. Corporate law practitioners and researchers argue that the affirmation of the “poison pill” made the second generation of antitakeover statutes redundant, while empirical scholars in corporate finance continue to find wide-ranging impacts from their adoption. This paper subjects the standard approach used in the empirical literature to a series of straightforward extensions using current best practice in panel data analysis. Contrary to the majority of published research, there is scant evidence for a consistent and reliable impact of antitakeover statute adoption on firm activity. These findings follow logically from the legal argument that takeover statutes provide little additional takeover deterrence in the presence of a “shadow pill.”"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html#important-figure",
    "href": "research/working_papers/antitakeover/index.html#important-figure",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Important figure",
    "text": "Important figure\nFigure 10 reports the event study estimates of the impact of poison pill law changes using the estimator from Callaway and Sant’Anna (2020) and the data and design changes described in Section 5.1. Model 1 includes only the fixed effects without any covariates, Model 2 includes the covariates in the short regression model from Karpoff and Wittry (2018), and Model 3 includes their full model.\n\n\n\nFigure 10"
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html",
    "href": "research/working_papers/ml_comp/index.html",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "",
    "text": "Paper\nSSRN"
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html#abstract",
    "href": "research/working_papers/ml_comp/index.html#abstract",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "Abstract",
    "text": "Abstract\nThis paper challenges conventional methods used in financial valuation across areas in litigation practice. We use a large-scale empirical simulation, using real firm data, to demonstrate that the widely used “comparable companies” approach allows enormous expert discretion, which enables substantial inconsistency and subjective judgment in court valuations. We then use the same simulation approach to show that using better data choices together with contemporary penalized regression methods generates valuation estimates that are considerably less variable, thereby reducing the scope for expert bias. We also apply this approach to a recent Delaware valuation dispute. This paper should transform financial valuation practice in litigation by both diagnosing and offering a cure for excessive discretion and variability in valuation disputes. Our methods would lead to better performing and more empirically grounded outcomes in legal disputes involving valuation, thus enhancing the fairness and efficiency of the judicial processes in valuation litigation."
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html#important-figure",
    "href": "research/working_papers/ml_comp/index.html#important-figure",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "Important figure",
    "text": "Important figure\nThis figure reports the valuation estimates for Landstar \\(d^*\\) = 43 trading days after the end of 2017 Q4 using different input combinations. The top panel plots the estimated market value from each of the 24 combinations, which are represented by the grey tiles in the lower panel. The combinations vary based on the choice of matching feature set, number of matches, distance measure, and summary measure.\n\n\n\nFigure 1"
  },
  {
    "objectID": "research/articles/bagel/index.html",
    "href": "research/articles/bagel/index.html",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/bagel/index.html#abstract",
    "href": "research/articles/bagel/index.html#abstract",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "Abstract",
    "text": "Abstract\nWe investigate the use of machine learning (ML) and other robust estimation techniques in event studies conducted on single securities for the purpose of securities litigation. Single-firm event studies are widely used in civil litigation, with billions of dollars in settlements hinging on the outcome of the exercise. We find that regularization (equivalently, penalized estimation) can yield noticeable improvements in both the variance of event-date abnormal returns and significance-test power. Thus we believe that there is a role for ML methods in event studies used in securities litigation. At the same time, we find that ML-induced performance improvements are smaller than those based on other good practices. Most important are (i) the use of a peer index based on returns for firms in similar industries (how this is computed appears to be less important than that some version be included), and (ii) for significance testing, using the SQ test proposed in Gelbach et al. (2013), because it is robust to the considerable non-normality present in abnormal returns."
  },
  {
    "objectID": "research/articles/bagel/index.html#important-figure",
    "href": "research/articles/bagel/index.html#important-figure",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "Important figure",
    "text": "Important figure\nFigure 1 plots specification ranks. Each specification has 8 MSE performance values: two time periods (1999–2009 vs. 2009–2019), with and without the FFC factors, and two MSE normalization approaches (\\(\\widehat{R}_{oos}^{k}\\) and \\(\\widehat{R}_{het}^k\\), described below). Each gray dot represents a rank from 1 to 11, and each rank is represented once for each of the eight time-period/FFC-factor/MSE metric combinations. The diamonds plot the specifications’ average ranks. Blue diamonds signify models that allow firms to enter the regression function individually and use cross-validation and penalized regression; red diamonds represent specifications that do not.\n\n\n\nFigure 1\n\n\n@article{baker2020machine,\n  title={Machine learning and predicted returns for event studies in securities litigation},\n  author={Baker, Andrew and Gelbach, Jonah B and others},\n  journal={Journal of Law, Finance, and Accounting},\n  volume={5},\n  number={2},\n  pages={231--272},\n  year={2020},\n  publisher={Now Publishers, Inc.}\n}"
  },
  {
    "objectID": "research/articles/dual_class/index.html",
    "href": "research/articles/dual_class/index.html",
    "title": "Dual-Class Index Exclusion",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/dual_class/index.html#abstract",
    "href": "research/articles/dual_class/index.html#abstract",
    "title": "Dual-Class Index Exclusion",
    "section": "Abstract",
    "text": "Abstract\nOne of the most contentious and long-standing debates in corporate governance is whether company founders and other insiders should be permitted to use multi-class stock structures with unequal votes to control their companies while seeking capital through a public listing. Stymied by the permissive attitudes of legislatures and regulators, institutional investors opposed to multi-class arrangements recently turned to a new potential source of regulation: benchmark equity index providers. At the behest of institutional investors, the three largest index providers recently changed the eligibility requirements for their benchmark equity indexes to exclude, limit or underweight companies with multi-class stock structures. Investors expected the prospect of exclusion from such indexes to discourage founders and directors from adopting dual-class stock structures in connection with their initial public offerings.\nWhile there is a voluminous financial literature on the effects of index inclusion and exclusion on stock prices, and legal scholars have recently explored the corporate governance implications of the exponential growth of passive index investing, focusing primarily on the incentives of index fund asset managers, neither the financial nor the legal literature have considered the corporate governance role and influence of the parties who write the rules for index investing: the index providers. We begin to fill this gap in the literature by assessing the efficacy of index providers as corporate governance arbiters through the rubric of their dual-class index exclusion decisions.\nWe start with the premise that the index exclusion sanction will not discourage dual-class listings unless it is sufficiently costly to outweigh the perceived benefits of founder control through a multi-class stock structure. We expect the index exclusion sanction will not be sufficiently costly for several reasons. First, it is difficult, if not impossible, to implement a sanction through the public capital markets. Second, the index inclusion effect on which the anticipated sanction is premised has effectively disappeared in recent years and may never have been a long-term source of lower capital costs. Third, despite the explosive growth of index investing in recent years, funds following stock indexes still hold a relatively modest percentage of the market capitalization of U.S. equities – around 12% according to BlackRock. Finally, the proliferation of index investing opportunities has weakened the market-moving influence of any one benchmark index.\nTo test the efficacy of the sanction, we conduct an event study of the S&P announcement that dual-class companies would henceforth be excluded from the S&P 1500 Composite Index and its components – the S&P 500, S&P 400 mid-cap and S&P 600 small-cap indices. Because S&P grandfathered dual-class companies currently in the index, we are able to compare movements in the stock prices of dual-class companies currently in the index with movements in the stock prices of dual-class companies not yet included in the index at the time of announcement. We do not observe any statistically significant abnormal returns in the stock prices of either included or excluded firms as a result of the S&P announcement, suggesting that exclusion is not expected to have a significant adverse cost of capital effect on firms that elect to list with a dual-class stock structures in the future and the sanction is ineffective. In the absence of an effective sanction, the exclusion of dual-class shares from benchmark equity indexes will not affect corporate governance choices. It may, however, have material adverse consequences for index investors and the index providers themselves."
  },
  {
    "objectID": "research/articles/dual_class/index.html#important-figure",
    "href": "research/articles/dual_class/index.html#important-figure",
    "title": "Dual-Class Index Exclusion",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1\n\n\n@article{winden2019dual,\n  title={Dual-Class Index Exclusion},\n  author={Winden, Andrew and Baker, Andrew},\n  journal={Va. L. \\& Bus. Rev.},\n  volume={13},\n  pages={101},\n  year={2019},\n  publisher={HeinOnline}\n}"
  },
  {
    "objectID": "research/articles/diversity_washing/index.html",
    "href": "research/articles/diversity_washing/index.html",
    "title": "Diversity Washing",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/diversity_washing/index.html#abstract",
    "href": "research/articles/diversity_washing/index.html#abstract",
    "title": "Diversity Washing",
    "section": "Abstract",
    "text": "Abstract\nWe provide large-sample evidence on whether U.S. publicly traded corporations use voluntary disclosures about their commitments to employee diversity opportunistically. We document significant discrepancies between companies’ external stances on diversity, equity, and inclusion (DEI) and their hiring practices. Firms that discuss DEI excessively relative to their actual employee gender and racial diversity (“diversity washers”) obtain superior scores from environmental, social, and governance (ESG) rating organizations and attract more investment from institutional investors with an ESG focus. These outcomes occur even though diversity-washing firms are more likely to incur discrimination violations and have negative human-capital-related news events. Our study provides evidence consistent with growing allegations of misleading statements from firms about their DEI initiatives and highlights the potential consequences of selective ESG disclosures."
  },
  {
    "objectID": "research/articles/diversity_washing/index.html#important-figure",
    "href": "research/articles/diversity_washing/index.html#important-figure",
    "title": "Diversity Washing",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Nonparametric summary of ESG outcomes by DEI discussion and diversity ranks. This figure presents the average of Refinitiv’s Overall Score (panel A), Refinitiv’s Social Score (panel B), Sustainalytics Overall Score (panel C), Sustainalytics Social Score (panel D), Ownership by US SIF funds (panel E), and Ownership by ESG funds, based on the fund name (panel F). Each heat map is broken down by the decile of DEI discussion (measured by the overall amount of DEI discussion in our corpus of SEC documents) and the decile of underlying diversity (measured by the percentage of a firm’s U.S.-based workforce that is either female or non-white)\n\n\n\nFigure 5\n\n\n@article{baker2022diversity,\n  title={Diversity washing},\n  author={Baker, Andrew C and Larcker, David F and McCLURE, CHARLES G and Saraph, Durgesh and Watts, Edward M},\n  journal={Journal of Accounting Research},\n  year={2022},\n  publisher={Wiley Online Library}\n}"
  },
  {
    "objectID": "research/articles/staggered_did/index.html",
    "href": "research/articles/staggered_did/index.html",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/staggered_did/index.html#abstract",
    "href": "research/articles/staggered_did/index.html#abstract",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Abstract",
    "text": "Abstract\nWe explain when and how staggered difference-in-differences regression estimators, commonly applied to assess the impact of policy changes, are biased. These biases are likely to be relevant for a large portion of research settings in finance, accounting, and law that rely on staggered treatment timing, and can result in Type-I and Type-II errors. We summarize three alternative estimators developed in the econometrics and applied literature for addressing these biases, including their differences and tradeoffs. We apply these estimators to re-examine prior published results and show, in many cases, the alternative causal estimates or inferences differ substantially from prior papers."
  },
  {
    "objectID": "research/articles/staggered_did/index.html#important-figure",
    "href": "research/articles/staggered_did/index.html#important-figure",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Important figure",
    "text": "Important figure\nThe top panel of Fig. 3 illustrates the diagnostic test for Simulations 4, 5, and 6. Because the diagnostic test only applies to balanced panels, in constructing this figure our simulation is modified to artificially induce a balanced panel of firm-year observations from Compustat before drawing fixed effects and residuals from the empirical distribution.\n\n\n\nFigure 3\n\n\n@article{baker2022much,\n  title={How much should we trust staggered difference-in-differences estimates?},\n  author={Baker, Andrew C and Larcker, David F and Wang, Charles CY},\n  journal={Journal of Financial Economics},\n  volume={144},\n  number={2},\n  pages={370--395},\n  year={2022},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "research/articles/slr/index.html",
    "href": "research/articles/slr/index.html",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "",
    "text": "Paper\nCode\nData"
  },
  {
    "objectID": "research/articles/slr/index.html#abstract",
    "href": "research/articles/slr/index.html#abstract",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "Abstract",
    "text": "Abstract\nLawsuits brought pursuant to section 10(b) of the Securities and Exchange Act depend on the reliability of a statistical tool called an event study to adjudicate issues of reliance, materiality, loss causation, and damages. Although judicial acceptance of the event study technique is pervasive, there has been little empirical analysis of the ability of event studies to produce reliable results when applied to a single company’s security.\nUsing data from the recent financial crisis, this Note demonstrates that the standard-model event study used in most court proceedings can lead to biased inferences sanctioned through the Daubert standard of admissibility for expert testimony. In particular, in the presence of broad market volatility, a base event study will cause too many returns to be identified as statistically significant. Even recently proposed variations of the event study model specifically designed to address violations of the statistical assumptions of an event study will not completely correct this bias.\nThis Note proposes two alternative forms of event studies that are capable of creating statistically reliable results and should be adopted by courts in instances where there is cause to believe that market volatility has increased. Over previous decades, the judiciary has steadily moved toward a reliance on empirics and expert testimony in overseeing complex civil cases. Yet there has been surprisingly little research accompanying this judicial deference on the ability of statistical evidence to produce the promised result. This Note calls into question whether this movement has been beneficial from a logical or empirical perspective, but it demonstrates that alternative techniques that can aid the finder of fact in resolving these disputes—regardless of market trends—may in fact exist."
  },
  {
    "objectID": "research/articles/slr/index.html#important-figure",
    "href": "research/articles/slr/index.html#important-figure",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 3\n\n\n@article{baker2016single,\n  title={Single-firm event studies, securities fraud, and financial crisis: problems of inference},\n  author={Baker, Andrew C},\n  journal={Stan. L. Rev.},\n  volume={68},\n  pages={1207},\n  year={2016},\n  publisher={HeinOnline}\n}"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Working Paper\n\n\n\n\n\n\n2024\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n\n2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n\n2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Research",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n\n\n  \n\n\n\n\nDiversity Washing\n\n\n\n\n\nJournal of Accounting Research 2024 (Joint with David F. Larcker, Charles G. McClure, Durgesh Saraph, and Edward M. Watts)\n\n\n\n\n\n\n2024\n\n\n\n\n\n\n  \n\n\n\n\nHow Much Should We Trust Staggered Difference-in-Differences Estimates?\n\n\n\n\n\nJournal of Financial Economics 22(2) 2022 (Joint with David F. Larcker and Charles C.Y. Wang)\n\n\n\n\n\n\n2022\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning and Predicted Returns for Event Studies in Securities Litigation\n\n\n\n\n\nJournal of Law, Finance, and Accounting 5(2) 2020 (Joint with Jonah B. Gelbach)\n\n\n\n\n\n\n2020\n\n\n\n\n\n\n  \n\n\n\n\nDual-Class Index Exclusion\n\n\n\n\n\nVirginia Law & Business Review  13(2) 2019 (Joint with Andrew Winden)\n\n\n\n\n\n\n2019\n\n\n\n\n\n\n  \n\n\n\n\nSingle-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference\n\n\n\n\n\nStanford Law Review 68 2016\n\n\n\n\n\n\n2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/posts/DiD/index.html",
    "href": "posts/posts/DiD/index.html",
    "title": "Difference-in-Differences Methodology",
    "section": "",
    "text": "In this methodological section I will explain the issues with difference-in-differences (DiD) designs when there are multiple units and more than two time periods, and also the particular issues that arise when the treatment is conducted at staggered periods in time.\nIn the canonical DiD set-up (e.g. the Card and Kreuger minimum wage study comparing New Jersey and Pennsylvania) there are two units and two time periods, with one of the units being treated in the second period. Graphically, you can think of the relationship as the one presented below:\n\n\n\n\n\n\n\n\n\nUnder the parallel trends assumption that, ceteris paribus, the two units would have the same change in the outcome variable, the treatment effect is equal to the difference in the outcome variable for the treated unit in period 2 (\\(Y_2^T\\)) and the treated unit in period 1 (\\(Y_1^T\\)), less the same difference in the control unit (\\(Y_2^C - Y_1^C\\)).\n\n\n\n\n\n\n\n\n\nIt can be easily shown that the treatment effect (hereafter \\(\\tau\\)) shown in the above plot is equvalent to the coefficient on \\(\\delta\\) from the regression \\[\\tag{1} Y_{it} = \\gamma_t POST_t + \\gamma_i TREAT_i + \\delta POST_t \\times TREAT_i + \\epsilon_{it}\\] where \\(POST_t\\) is an indicator for being in the second period and \\(TREAT_i\\) is an indicator for the treated unit."
  },
  {
    "objectID": "posts/posts/DiD/index.html#summary",
    "href": "posts/posts/DiD/index.html#summary",
    "title": "Difference-in-Differences Methodology",
    "section": "Summary",
    "text": "Summary\nWhile the 2x2 DiD treatment effect can easily be calculated in (1), most DiD applications exploit variation across groups and units that receive treatment at different points in time. The coefficient that comes from the two-way fixed effects (TWFE) estimator when there are more than two units and periods is not an easily interperable parameter in the same manner. Numerous papers have now documented that this coefficient is in fact a weighted average of many different treatment effects, and that these weights are often negative and non-intuitive.\nFor the purposes of explaining the methodological issue I will focus on one paper’s derivation - Goodman-Bacon (2019) [hereinafter GB (2019)] (Multiple papers have derived this result with varying terminology. I will go over the remedies proposed by some of these papers below). According to GB (2019), the general estimator from the TWFE approach is actually a “weighted average of all possible two-group/two-period DiD estimators in the data.”\nUnder the TWFE approach, researchers estimate the regression with dummies for cross-sectional units (\\(\\alpha_i\\)) and time periods (\\(\\alpha_t\\)) and a treatment dummy (\\(D_{it}\\)). The POST and TREAT variables are thus subsumed by the fixed effects structure. This regression is generally written as:\n\\[\n\\tag{2} y_{it} = \\alpha_i + \\alpha_t + \\beta^{DD} D_{it} + \\epsilon_{it}\n\\]\nAs explained in GB (2019), we know relatively little about what the TWFE DiD value (\\(\\beta^{DD}\\)) measures when treatment timing varies, including how it compares mean outcomes across groups and when alternative specifications will work or why they change the estimates. What the GB (2019) derivation shows is that we can think of the TWFE estimator as a weighted average of individual 2x2 DiD estimators, with the weights proportional to group sizes and the variance of the treatment indicators in each pair, which is highest for units treated in the middle of the panel. In addition, GB (2019) shows how some of the 2x2 estimators use units treated at a particular time as the treatment group and never-treated units as a control group, while other use units treated at two different times, with later-treated groups being used as a control before treatment begins, and the earlier-treated group being used as control after its treatment begins.\nFurthermore, GB (2019) shows that when the treatment effects do not change over time, \\(\\beta^{DD}\\) is the variance-weighted average of cross-group treatment effects, and all of the weights are positive. However, when the treatment effect does vary across time, some of these 2x2 estimates enter the average with negative weights. This is because already-treated units act as controls, and changes in a portion of their treatment effects over time are subtracted from the DiD estimate."
  },
  {
    "objectID": "posts/posts/DiD/index.html#derivation",
    "href": "posts/posts/DiD/index.html#derivation",
    "title": "Difference-in-Differences Methodology",
    "section": "Derivation",
    "text": "Derivation\nAssume going forward that we have a balanced panel data set with \\(T\\) periods \\(t\\) and \\(N\\) cross-sectional units \\(i\\) that belong to either an untreated group \\(U\\), an early treatment group \\(k\\) which receives binary treatment at \\(t_k^*\\), and late treatment group \\(l\\) that receives a binary treatment at \\(t_l^* > t_k^*\\). For each group \\(a\\), it’s sample share is \\(n_a\\) and the share of time it spends treated is \\(\\overline{D}_a\\). Denote \\(\\overline{y}_b^{POST(a)}\\) to be the sample mean of \\(y_{it}\\) for units in group \\(b\\) during group \\(a\\)’s post period \\([t_a^*, T]\\), with \\(\\overline{y}_b^{PRE(a)}\\) defined similarly.\nFirst, I’ll replicate the figure from GB (2019) that shows how \\(\\beta^{DD}\\) is calculated when there are just the three groups described above - \\(U\\), \\(k\\), and \\(t\\). Here \\(U\\) is never treated, \\(k\\) the early treatment group is treated at \\(t_k^* = \\frac{34}{100} T\\) and the late treatment group \\(l\\) receives treatment at \\(t_l^* = \\frac{85}{100}T\\). We denote three sub-periods, the pre-period for group \\(k\\) = \\(PRE(k) = [0, t_k^* - 1]\\), the middle period when group \\(k\\) is treated but group \\(l\\) is not \\(MID(k, l) = [t_k^*, t_l^* - 1]\\), and the post-period for group \\(l\\) = \\(POST(l) = [t_l^*, T]\\). We assume that the treatment effect is equal to 10 for group \\(k\\) and 15 for group \\(l\\).\n\n\n\n\n\n\n\n\n\nAs GB (2019) note, the challenge is to show how the estimates from the TWFE estimation in Equation 2 map to the groups and times depicted in the figure above. GB (2019) shows that in the three group case here, we could form four 2x2 DiDs that can be estimated by Equation 1 on the subsamples of groups and times. I demonstrate the four possible 2x2 designs in the figure below.\n\n\n\n\n\n\n\n\n\nIn Panels A and B we see that if we consider only one treatment group and the untreated group, the two way fixed effects estimate reduces to the standard 2x2 DiD with the estimate equal to: \\[\\hat{\\beta}_{jU}^{2x2} = \\left(\\overline{y}_j^{POST(j)} - \\overline{y}_j^{PRE(j)} \\right) - \\left(\\overline{y}_U^{POST(j)} - \\overline{y}_U^{PRE(J)} \\right), j = k, l \\] However, if there are no untreated units in the sample, the TWFE estimator is only estimated by the difference in the timing of the treatments between treatment groups. This is represented in Panel C, where before \\(t_l^{*}\\), the early unit \\(k\\) acts as the treatment group and the later treated unit \\(l\\) acts as a control. The 2x2 DiD estimator compares the differences in outcome between the window when treatment status varies (\\(MID(k, l)\\)) and the early group’s pre-period (\\(PRE(k)\\)):\n\\[\n\\hat{\\beta}_{kl}^{2x2, k} = \\left(\\overline{y}_k^{MID(k, l)} - \\overline{y}_k^{PRE(k)} \\right) - \\left(\\overline{y}_l^{MID(k, l)} - \\overline{y}_l^{PRE(k)} \\right)\n\\] Panel D shows the opposite situation, where the later group changes treatment after \\(t_l^{*}\\) and the earlier treated unit \\(k\\) acts as the control. Again, the 2x2 compares average outcomes between two periods, here \\(POST(l)\\) and \\(MID(k, l)\\):\n\\[\n\\hat{\\beta}_{k, l}^{2x2, l} = \\left(\\overline{y}_l^{POST(l)} - \\overline{y}_l^{MID(k, l)} \\right) - \\left(\\overline{y}_k^{POST(l)} - \\overline{y}_k^{MID(k, l)} \\right)\n\\]\nWhat’s important to note here is that the already-treated unit \\(k\\) acts as a control even though they are treated, because their treatment assignment indicator does not change over the relevant period. In addition, each one of the four 2x2 estimates in the figure above uses only a fraction of the available data. The DiDs with treated and untreated units use the entire time period, but only for the two respective groups, so their sample shares are (\\(n_k + n_U\\)) and (\\(n_l + n_U\\)). The timing indicator DiDs (Panels C and D) also use only the observations from the two groups, but also use only a portion of the available time periods. \\(\\hat{\\beta}_{k,l}^{2x2, k}\\) only uses group \\(l\\)’s pre-period, so it’s sample share is \\((n_k + n_l)(1 - \\overline{D}_l)\\), while \\(\\hat{\\beta}_{kl}^{2x2, l}\\) uses group k’s post-period, so it’s share is \\((n_k + n_l)\\overline{D}_k\\).\nIn addition, each of the 2x2 DiDs are identified by the treatment indicator variation in the subsample over which it is estimated, and this varies by subsample:\n\\[\\widehat{V}_{jU}^D = n_{jU}(1 - n_{JU})\\overline{D}_j(1 - \\overline{D}_j), \\hspace{0.5cm} j = k, l\\]\n\\[\\widehat{V}_{kl}^{D, k} = n_{kl}(1 - n_{kl}) \\frac{\\overline{D}_k - \\overline{D}_l}{1 - \\overline{D}_l} \\frac{1 - \\overline{D}_k}{1 - \\overline{D}_l}\\]\n\\[\\widehat{V}_{kl}^{D, l} = n_{kl}(1 - n_{kl}) \\frac{\\overline{D}_l}{\\overline{D}_k} \\frac{\\overline{D}_k - \\overline{D}_l}{\\overline{D}_k}\\]\nwhere \\(n_{ab} \\equiv \\frac{n_a}{n_a + n_b}\\) is the relative size of groups within each comparison group. The first portion of each variance measure is the concentration, or total size of the groups, while the second portion comes from when the treatment occurs in each sample, and is the variance of the treatment indicator variable scaled by the size of the relevant window. The central result from GB (2019) is that any TWFE estimator is just an average of the 2x2 estimators in Panels A-D, with weights that are based on subsample shares \\(n\\) and variances \\(\\hat{V}\\).\nWith \\(K\\) timing groups, you can form \\(K^2 - K\\) timing only estimates comparing earlier and later treated groups. With an untreated group \\(U\\) you could form \\(K\\) treated/untreated 2x2 DiDs for a total of \\(K^2\\) DiD estimates. The weights on each of these 2x2 estimates used to construct \\(\\beta^{DD}\\) combine the absolute size of the subsample, the relative size of the treatment and control groups in the subsample, and the timing of the treatment in the subsample.\nTo put this in context, we can derive the weights for \\(\\beta^{DD}\\) from the example in the figures above. \\(t_k^*\\) and \\(t_l^*\\) were set so that \\(\\overline{D}_k = 0.66\\) and \\(\\overline{D}_l = 0.16\\). For the 2x2 DiDs in Panels A and B the weights given to the 2x2 DiD estimate for the earlier treated group, \\(s_{kU}\\) is greater than the weight given to the DiD for the later-treated group, \\(s_{lU}\\) because \\(k\\) is treated closer to the middle of the panel and has a higher treatment-indicator variance. This is also true for the timing-only 2x2 DiD’s (Panels C and D), where the weights are higher for the earlier treated groups both because it uses more data and because it has a higher treatment variance. If we calculate the weights for our four estimates here we get:\n\n\n\n\n \n  \n    weights \n    value \n  \n \n\n  \n    $s_{kU}$ \n    0.37 \n  \n  \n    $s_{lU}$ \n    0.22 \n  \n  \n    $s_{kl}^k$ \n    0.28 \n  \n  \n    $s_{kl}^l$ \n    0.13 \n  \n\n\n\n\n\nWhat is clear from the derivation is that panel length alone can change the DiD estimates substantially, even when each 2x2 DiD estimate is constant. This seems normatively undesirable. In addition, the weights assigned to each 2x2 estimate when aggregating through TWFE OLS are a result of the size of the subsample and the magnitude of the treatment variance. Groups treated closer to the middle of the panel get more weight, which again isn’t intuitively desirable."
  },
  {
    "objectID": "posts/posts/DiD/index.html#goodman-bacon-2019",
    "href": "posts/posts/DiD/index.html#goodman-bacon-2019",
    "title": "Difference-in-Differences Methodology",
    "section": "Goodman-Bacon (2019)",
    "text": "Goodman-Bacon (2019)\nGB (2019) proposes a series of diagnostic tests to examine the robustness of the TWFE DiD estimate. First, given the decomposition of the individual 2x2 DiD weights, GB (2019) proposes a more effective balancing test for staggered DiD. Existing approaches for evaluating the balance in a timing design involve estimating a linear relationship between a confounded \\(x_{it}\\) and the treatment timing \\(t_k^*\\), or comparing means between early and late treated groups. However, because control groups include both early and late-treated units, this does not test whether any such imbalance is likely to bias \\(\\beta^{DD}\\). Rather, GB (2019) proposes a single t-test of reweighted balance in \\(x_{it}\\), which is a proxy for whether \\(VWCT = 0\\):\n\nGenerate a dummy for whether a unit is an “effective treatment group”, i.e. whether a unit’s weight as a treatment group exceeds its weight as a control group, \\(B_k = w_k^T - w_k^C \\geq 0\\).\nRegressing timing-group means, \\(\\overline{x}_k\\) on \\(B_k\\), weighting by \\(\\left|w_k^T - w_k^C\\right|\\).\nThe coefficient on \\(B_k\\) then equals the covariate diferences weighted by the actual identifying variation, and the \\(t\\)-statistic tests against the null of no imbalance. You could also test for trends in the confounders by interacting year dummies or a linear trend with \\(B_k\\).\n\nIn addition, GB (2019) shows how to test the stability of the DiD coefficients by plotting each 2x2 DiD against its weight \\(w_k^*\\). You can calculate a variety of different conditional expectations over the subgroups, including the average effect and total weights for treated/untreated comparisons, and late/early treatment and early/late treatment comparisons. Adding the weights on the timing-based coefficients from the decomposition show how much of \\(\\beta^{DD}\\) comes from timing variation. Finally, you can identify influential observations by comparing the weights, and calculate what percentage of the total 2x2 coefficients drive 50% of the estimate, or whether influential observations have substantially different levels of \\(\\tau\\)."
  },
  {
    "objectID": "posts/posts/DiD/index.html#callaway-and-santanna-2018",
    "href": "posts/posts/DiD/index.html#callaway-and-santanna-2018",
    "title": "Difference-in-Differences Methodology",
    "section": "Callaway and Sant’Anna (2018)",
    "text": "Callaway and Sant’Anna (2018)\nCallaway and Sant’Anna (2018) [hereinafter CS (2018)] also consider the identification and estimation of treatment effect parameters using DiD with multiple time periods, variation in treatment timing, and where the parallel trends assumption may only hold after conditioning on observables. They propose a two-step estimation strategy with a bootstrap procedure to conduct asymptotically valid inference which can adjust for autocorrelation and clustering.\nCS (2018) define the causal parameters of interest in a staggered DiD framework as functionals of the ATE for group \\(g\\) at time \\(t\\), where a group is defined by when units are first treated (e.g. all firms treated in 2006, 2009, etc.). These causal parameters are called “group-time average treatment effects”. This setting allows you to aggregate the treatment effects by either relative time (i.e. the event study approach) or by calendar time.\nTo keep the same notation as in CS (2018), assume there are \\(\\mathcal{T}\\) periods where \\(t = 1, \\ldots, \\mathcal{T}\\), with \\(D_t\\) a binary variable equal to 1 if an individual is treated and 0 otherwise. In addition, define \\(G_g\\) to be a binary variable that is equal to 1 when an individual is first treated in period \\(g\\), and \\(C\\) as a binary variable equal to 1 for never-treated units. For each unit, exactly one of \\(G_g\\) or \\(C\\) is equal to 1. Denote the generalized propensity score as \\(p_g(X) = P(G_g = 1 | X, G_g + C = 1)\\), which is the probability that an individual is treated conditional on having covariates \\(X\\) and conditional on being a member of a group \\(g\\) or a control group \\(C\\).\nCS (2018) frames its identification strategy within the potential outcomes framework. Let \\(Y_t(1)\\) and \\(Y_t(0)\\) be the potential outcomes at time \\(t\\) with and without treatment, respectively. The observed outcome in each period can thus be expressed as \\(Y_t = D_t Y_t(1) + (1 - D_t)Y_t(0)\\). The authors focus on the average treatment effect for individuals first treated in period \\(g\\) at time period \\(t\\), called the group-time average treatment effect, denoted by: \\[ATT(g, t) = \\mathbb{E}[Y_t(1) - Y_t(0) | G_g = 1]\\] The assumptions in the CS (2018) framework are of random sampling, parallel trends conditional on covariates, irreversability of treatment (after a unit is treated it is treated for the remainder of the panel), and overlap (the propensity scores are greater than 0 and less than 1).\nGiven the parameter of interet \\(ATT(g, t)\\), CS (2018) develop a non-parametric identification strategy for the group-time average treatment effect, which allows for treatment effect heterogeneity and does not make functional form assumptions about the evolution of potential outcomes. Under the assumptions above, the group-time average treatment effect is nonparametrically identified as: \\[ATT(g, t) = \\mathbb{E} \\left[\\left( \\frac{G_g}{\\mathbb{E}[G_g]} - \\frac{\\frac{p_g(X)C}{1 - p_g(X)}}{\\mathbb{E}\\left[\\frac{p_g(X)C}{1 - p_g(X)} \\right]} \\right) \\left(Y_t - T_{g - 1}\\right)\\right]\\] This is just a simple weighted average of the “long difference” of the outcome variable, with the weights depending on the propensity score, which are normalized to one. The intuition is to take observations from the control group and group \\(g\\), omitting other groups, and then up-weight observations from the control group that have characteristics similar to those frequently found in group \\(g\\) and down-weight observations from the control group that are rarely in group \\(g\\). This reweighting ensures that the covariates of the treatment and control group are balanced. The authors also explain in an appendix how to use their reweighting scheme where future-treated units can be used as controls.\nWith \\(ATT(g, t)\\) in hand we can aggregate the group-time treatment effects into fewer intrepetable causal effect parameters, which makes interpretation easier, and also increases statistical power and reduces estimation uncertainty. CS (2018) propose doing the following:\n\nAggregating \\(ATT(g, t)\\) by timing-group, \\(\\tilde{\\theta}_s(g) = \\frac{1}{\\mathcal{T} - g + 1} \\sum_{t = 2}^{\\mathcal{T}} 1 \\{g \\leq t\\} ATT(g, t)\\), and combining the group average treatment effect by the size of each group \\(\\theta_s = \\sum_{g = 2}^{\\mathcal{T}}\\tilde{\\theta}_s(g) P(G = g)\\).\nAggregating \\(ATT(g, t)\\) by the length of exposure to treatment (denoted \\(e\\)) to test whether there are dynamic treatment effects (i.e. the treatment effect is explicitly a function of the time since treatment). They consider \\(\\tilde{\\theta}_D(e) = \\sum_{g = 2}^{\\mathcal{T}} \\sum_{t = 2}^{\\mathcal{T}} 1 \\{t - g + 1 = e\\} ATT(g, t)P(G = g | t - g + 1 = e)\\). Here \\(\\tilde{\\theta}_D(1)\\) would be equal to the average (based on group size) treatment effect in one (\\(e = 1\\)) period. This aggregation gives you a properly weighted event-study similar to pre-existing practice, which might be desirable for continuity. In addition, you could then average over all possible values of \\(e\\) to get \\(\\theta_D = \\frac{1}{\\mathcal{T} - 1} \\sum_{e = 1}^{\\mathcal{T} - 1} \\tilde{\\theta}_D(e).\\)\nAggregating \\(ATT(g, t)\\) by calendar time involves computing an average treatment effect for all individuals that are treated in period \\(t\\) and then averaging across all periods. Here, \\(\\tilde{\\theta}_C(t) = \\sum_{g = 2}^{\\mathcal{T}} 1 \\{g \\leq t\\} ATT(g, t) P(G = g | g \\leq t)\\). \\(\\tilde{\\theta}_C(t)\\) can be interepreted as the average treatment effect in period \\(t\\) for all groups treated by period \\(t\\), and you can further aggregate to \\(\\theta_C = \\frac{1}{\\mathcal{T} - 1} \\sum_{t = 2}^{\\mathcal{T}} \\tilde{\\theta}_C(t)\\), which would be the average treatment effect when calendar time matters. This final summary statistic naturally puts the most weight on groups that are treated in the earliest periods, because they enter more of the \\(ATT(g, t)\\) estimates.\nAggregating \\(ATT(g, t)\\) in the case where the timing of treatment matters, and where there are dynamic treatment effects (probably the most likely reality in real world scenarios). Here CS (2018) considers dynamic treatment effects only for \\(e \\leq e^{'}\\) and for groups with at least \\(e^{'}\\) periods of post-treatment data, which removes the impact of selective treatment timing by keeping the same set of groups across all values of \\(e\\). There is a tradeoff here between the amount of groups you have in your final estimation sample (larger with smaller values of \\(e^{'}\\)) and the length of the exposure that you can estimate (smaller with smaller values of \\(e^{'}\\)). Let \\(\\delta_{gt}(e, e^{'}) = 1\\{t - g + 1 = e \\}1\\{T - g + 1 \\geq e^{'}\\}1\\{e \\leq e^{'}\\}\\). Thus \\(\\delta_{gt}(e, e^{'})\\) is equal to one in the period where group \\(g\\) has been treated for exactly \\(e\\) periods, if group \\(g\\) has at least \\(e^{'}\\) post-treatment periods available, and if the length of exposure \\(e\\) is less than the post-treatment period requirement \\(e^{'}\\). The average treatment effect for groups in \\(\\delta_{gt}(e, e^{'})\\) is given by \\(\\tilde{\\theta}_{SD}(e, e^{'}) = \\sum_{g = 2}^{\\mathcal{T}} \\sum_{t = 2}^{\\mathcal{T}} \\delta_{gt}(e, e^{'}) ATT(g, t)P(G = g | \\delta_{gt}(e, e^{'}) = 1)\\). With \\(\\tilde{\\theta}_{SD}(e, e^{'})\\) we can calculate the average treatment effect for groups with at least \\(e^{'}\\) periods of post-treatment data as \\(\\theta_{SD}(e^{'}) = \\frac{1}{\\mathcal{T} - e^{'}} \\sum_{e = 1}^{\\mathcal{T} - e^{'}} \\tilde{\\theta}_{SD}(e, e^{'})\\)."
  },
  {
    "objectID": "posts/posts/DiD/index.html#abraham-and-sun-2019",
    "href": "posts/posts/DiD/index.html#abraham-and-sun-2019",
    "title": "Difference-in-Differences Methodology",
    "section": "Abraham and Sun (2019)",
    "text": "Abraham and Sun (2019)\nThis paper is tightly linked with the Callaway and Sant’Anna paper, but focuses exclusively on the event-study context, where you include leads and lags of the treatment variable instead of a single binary indicator variable. The authors confirm that in the event study context, where the timing of treatment varies across units, lead/lag regressions can produce causally uninterpretable results because they assign non-convex weights to cohort-specific treatment effects. Their proposed method estimates the dynamic effect for each cohort (equivalent to group \\(G_g\\) from CS (2018)), and then calculates the average of cohort-specific estimates.\nIn the event study context, the TWFE regression with leads and lags of treatments takes the form of:\n\\[y_{it} = \\alpha_i + \\alpha_t + \\sum_{l = -K}^{-2} \\beta_l D_{it}^l + \\sum_{l = 0}^L \\beta_l D_it^l + \\epsilon_{it}\\].\nwhere \\(D_{it}^l\\) is an indicator for being \\(l\\) time periods relative to \\(i\\)’s initial treatment (treatment is \\(l\\) = 0), and \\(\\alpha_i\\) and \\(\\alpha_t\\) are unit and time fixed effects, as before. AS (2019) focus on the “cohort-specific average treatment effects on the treated” \\(l\\) periods from initial treatment. This is denoted \\(CATT_{el} = E\\left[Y_{i, e + l}^e - Y_{i, e + l}^{\\infty} | E_i = e \\right]\\) where \\(E_i\\) is the time period of initial treatment, and a cohort \\(e\\) is a set of units for which \\(E_i = e\\); \\(Y_{it}^{\\infty}\\) is the counterfactual outcome of unit \\(i\\) if it never received treatment.\nThe key theoretical result in this paper is that, even when doing an event-study estimation technique rather than a single binary indicator variable, the coefficients on the TWFE lead/lag indicators could be biased, because the weights assigned to the different \\(CATT\\)s are hard to interpret and need not be positive without assuming treatment effect homogeneity. Specifically, the FE estimands for \\(l\\) periods relative to treatment can be written as non-convex averages of not only \\(CATT_{e, l}\\) from that period, but also \\(CATT_{e, l^{'}}\\) from other periods. This is similar to the result in \\(GB (2019)\\) that \\(\\Delta ATT \\neq 0\\) with dynamic treatment effects, although (I believe) the event study framework does solve some of the odd variance-weighted issues brought up by GB (2019) in the binary indicator context.\nThe proposed alternative estimation technique in AS (2019) is to use an interacted specification that is saturated in relative time indicators \\(D_{it}^l\\) and cohort indicators \\(1\\{E_i = e\\}\\) to estimate each \\(CATT_{el}\\), which they call an “interaction-weighted” (IW) estimator. The DiD under the IW estimator is equivalent to the difference between the average change in outcomes for cohort \\(e\\), which is exactly \\(l\\) periods relative to treatment, and the average change for cohorts that have not been treated by \\(t = e + l\\) and is estimated simply by \\[Y_{it} = \\alpha_i + \\alpha_t + \\sum_e \\sum_{l \\neq -1} \\delta_{el}(1\\{E_i = e\\} \\cdot D_{it}^l) + \\epsilon_{it}\\] Finally, you can re-create the standard event-study plots by taking the weighted average over cohorts \\(e\\) for time period \\(l\\), with the weights equal to the share of each cohort in the relevant periods."
  },
  {
    "objectID": "posts/posts/DiD/index.html#imai-and-kim-2019",
    "href": "posts/posts/DiD/index.html#imai-and-kim-2019",
    "title": "Difference-in-Differences Methodology",
    "section": "Imai and Kim (2019)",
    "text": "Imai and Kim (2019)\nImai and Kim (2019) [hereinafter IK (2019)] also consider the impact of TWFE on DiD estimates with multiple periods, and where units can switch in and out of treatment at different periods in time (this is different than, for example, CS (2018) which assumes treatment continues for the rest of the panel). IK (2019) uses a matching framework to propose a multi-period DiD estimator which eliminates the bias in the TWFE estimator. Their matching estimator is equivalent to a weighted two-way fixed effects regression estimator.\nDefine a within-unit matched set as the group containing the observation of a treated unit from the previous time period if it is under the control condition (treat = \\(X_{it}\\) = 0) and to be an empty set otherwise: \\[\\mathcal{M}_{it} = \\{(i^{'}, t^{'}) : i^{'} = i, t^{'} = t - 1, X_{i^{'}t^{'}} = 0\\}\\] The within-time matched set is defined as a group of control observations in the same time period whose prior observations are also under the control condition: \\[\\mathcal{N}_{it} = \\{(i^{'}, t^{'}) : i^{'} \\neq i, t^{'} = t, X_{i^{'}t^{'}} = X_{i^{'}, t^{'} - 1} \\}\\] And the adjusted set, which contains the control observations in the previous period that share the same unit as those in \\(\\mathcal{N}_{it}\\): \\[ \\mathcal{A}_{it} = \\{(i^{'}, t^{'}) : i^{'} \\neq i, t^{'} = t - 1, X_{i^{'}t^{'}} = X_{i^{'}t} = 0 \\} \\]\nWith these matched sets and adjustment sets, the multi-period DiD estimator is the average of 2x2 DiD estimators applied whenever there is a change from the control condition to the treatment condition (Note, I believe this methodology just looks at one-period effects, so rules out dynamic treatment effects). Then the treatment effect is \\[\\hat{\\tau} = \\frac{1}{\\sum_{i = 1}^N \\sum_{t = 1}^T D_{it}} \\sum_{i = 1}^N \\sum_{t = 1}^T D_{it} \\left(\\widehat{Y_{it(1)}} - \\widehat{Y_{it(0)}} \\right)\\] where \\(D_{i1} = 0 \\forall i\\), \\(D_{it} = X_{it} \\cdot 1\\{|\\mathcal{M}_{it}| \\cdot |\\mathcal{N}_{it}| > 0\\}\\) for \\(t > 1\\) and for \\(D_{it} = 1\\) we define:\n\\[\\begin{equation}\n   \\widehat{Y_{it}(x)} = \\left\\{\n       \\begin{array}\n         YY_{it} & \\text{if } X_{it} = 1 \\\\\n         Y_{i, t- 1} + \\frac{1}{|\\mathcal{N}_{it}|} \\sum_{(i^{'}, t) \\in \\mathcal{N}_{it}} Y_{i^{'}t} -\n         \\frac{1}{|\\mathcal{A}_{it}|} \\sum_{(i^{'}, t^{'}) \\in \\mathcal{A}_{it}} Y_{i^{'}t^{'}} & \\text{if } X_{it} = 0\n       \\end{array}\n       \\right.\n\\end{equation}\\]\nIn words, this means that when the treatment status of a unit changes from a control (\\(X_{it} = 0\\)) at time \\(t-1\\) to a treatment (\\(X_{it} = 1\\)) at time \\(t\\), and there exists at least one other unit whose treatment status does not change in the same time periods, the counterfactual outcome for observation \\((i, t)\\) is estimated by substracting from the realized outcome \\(Y_{it}\\), its own observed previous period outcome \\(Y_{i, t-1}\\), as well as the average outcome difference between the same two time periods among the other units whose treatment status does not turn on in time \\(t\\).\nWith these sets and estimation strategy, the IK (2019) matching based DiD estimator is equivalent to the following weighted two-way fixed effect estimator:\n\\[\\hat{\\tau} = \\hat{\\beta}_{WFE2} =  \\underset{\\beta}{\\text{armin}} \\sum_{i = 1}^N \\sum_{t = 1}^T W_{it}\\{(Y_{it} - \\overline{Y}_i^* - \\overline{Y}_t^* + \\overline{Y}^*) - \\beta(X_{it} - \\overline{X}_i^* - \\overline{X}_t^* + \\overline{X}^*) \\}^2 \\] where asterisks indicate weighted averages and the weights \\(W_{it}\\) are given by:\n\\[W_{it} = \\sum_{i^{'} = 1}^N \\sum_{t^{'} - 1}^T D_{i^{'}t^{'}} \\cdot w_{it}^{i^{'}t^{'}}\\] and\n\\[\\begin{equation}\n   w_{it}^{i^{'}t^{'}} = \\left\\{\n       \\begin{array}\n         11 & \\text{if } (i, t) = (i^{'}, t^{'}) \\\\\n         1 / |\\mathcal{M}_{i^{'}t^{'}}| & \\text{if } (i, t) \\in \\mathcal{M}_{i^{'}t^{'}} \\\\\n         1 / |\\mathcal{N}_{i^{'}t^{'}}| & \\text{if } (i, t) \\in \\mathcal{N}_{i^{'}t^{'}} \\\\\n         (2 X_{it} - 1)(2X_{i^{'}t^{'}}) / |\\mathcal{A}_{i^{'}t^{'}} & \\text{if } (i, t) \\in \\mathcal{A}_{i^{'}t^{'}} \\\\\n         0 & \\text{otherwise}\n       \\end{array}\n       \\right.\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/posts/DiD/index.html#cengiz-dube-lindner-and-zipperer-2019",
    "href": "posts/posts/DiD/index.html#cengiz-dube-lindner-and-zipperer-2019",
    "title": "Difference-in-Differences Methodology",
    "section": "Cengiz, Dube, Lindner, and Zipperer (2019)",
    "text": "Cengiz, Dube, Lindner, and Zipperer (2019)\nCDLZ (2019) estimates the impact of minimum wage changes on low-wage jobs across a series of 138 prominent state-level minimum wage changes between 1979 and 2016 in the United States using a difference-in-differences approach. In Online Appendix D, CDLZ (2019) notes that there are issues in aggregating discrete DiD estimates through OLS, and as a robustness check seperates and plots the distribution of the individual treatment effect for each of the events.\nTo do this, the authors create 138 event \\(h\\)-specific datasets including the outcome variable and controls for the treated state \\(h\\) and all other “clean controls” that don’t have a material change to the state minimum wage within the eight year estimation window (\\(t = -3\\) to \\(t = 4\\)). For each event, they they then run a one-treated panel DiD based on their baseline strategy:\n\\[\nY_{sjth} = \\sum_{\\tau = -3}^4 \\sum_{k = -4}^4 \\alpha_{\\tau k h} \\mathcal{I}_{sjth}^{\\tau k} + \\mu_{sjh} + \\rho_{sjth} + u_{sjth}\n\\] I will ignore the specific subscripts on all the regression terms; in essence this is just a saturated panel DiD with a lead/lag treatment indicator \\(\\mathcal{I}\\) and state \\(\\mu\\) and time \\(\\rho\\) fixed effects.1 CDLZ (2019) then plots the distribution of the \\(\\alpha\\) treatment effects, along with their confidence intervals using the Ferman and Pinto (forthcoming) method for heteroskedastic robust cluster residual bootstrapping more appropriate for single-treated units.\nCDLZ (2019) also stacks the event-specific data sets to calculate an average effect across all 138 events using a single set of treatment effects. As the authors note, this is an alternative to a baseline TWFE DiD estimate, but “uses a more stringent criteria for admissable control groups, and is more robust to possible problems with a staggered treatment design in the presence of heterogeneous treatment effects.” By stacking and aligning events in event-time, this approach is equivalent to a setting where the events happen contemporaneously, and it prevents negative weighting of some events that may occur with a staggered design. Moreover, by dropping all control states with any state-level minimum wage increases within the 8 year event window, this method guards against bias due to heterogeneous treatment effects that show up in the \\(\\Delta ATT\\) term from GB (2019)."
  },
  {
    "objectID": "posts/posts/PNAS/index.html",
    "href": "posts/posts/PNAS/index.html",
    "title": "What Can We Say About Medical Marijuana and Opioid Overdose Mortality?",
    "section": "",
    "text": "In this post I expand on the implications of recent econometric work on issues with difference-in-difference (DiD) designs with staggered treatment rollout. For a longer discussion of these issues, and the details of new proposed modifications to the standard two-way fixed effect regression-based DiD models, refer to my prior post here. Here I will demonstrate the practical importance of correcting for these issues with staggered DiD on a live policy issue - whether the adoption of legalized medical cannabis laws has a causal effect on opioid overdose mortality.\nThe potential beneficial impact of legal medical marijuana has been debated since \\(\\color{blue}{\\text{Bachhuber et al. 2014}}\\). Using a staggered DiD design \\(\\color{blue}{\\text{Bachhuber et al. 2014}}\\) found that states with medical cannabis laws experienced a slower increase in opioid overdose mortality over the period 1999-2010. Given the recent opioid epidemic, and the accompanying increase in “deaths of despair” around the country, this result generated substantial academic and popular attention. At the time of this writing, the paper has been cited over 500 times since publication."
  },
  {
    "objectID": "posts/posts/PNAS/index.html#generalized-synthetic-control",
    "href": "posts/posts/PNAS/index.html#generalized-synthetic-control",
    "title": "What Can We Say About Medical Marijuana and Opioid Overdose Mortality?",
    "section": "Generalized Synthetic Control",
    "text": "Generalized Synthetic Control\n\n\nCode\n# estimate the generalized synthetic control m ethod\nout <- gsynth(ln_age_mort_rate2 ~ TREAT, data = data_no_ND, index = c(\"state\", \"Year\"),\n              force = \"two-way\", CV = TRUE, se = TRUE, nboots = 1000, cores = 4, r = c(0, 3), \n              min.T0 = 4)\n\n#plot\nout$est.att %>% \n  as_tibble(rownames = \"t\") %>% \n  mutate(t = as.numeric(t)) %>% \n  ggplot(aes(x = t, y = ATT)) + \n  geom_ribbon(aes(ymin = CI.lower, ymax = CI.upper), color = \"lightgrey\", alpha = 1/2) + \n  geom_line(color = \"red\") + \n  geom_point(color = \"red\") + \n  geom_hline(yintercept = 0) + \n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  theme(axis.title.x = element_blank()) + \n  theme_bw() + \n  theme(axis.title.x = element_blank())\n\n\n\n\n\nThis seems to suggest a short term positive treatment effect, followed by a potential reversal, although the number of states with many post-treatment years is small so the confidence intervals are large."
  },
  {
    "objectID": "posts/posts/log_age/index.html",
    "href": "posts/posts/log_age/index.html",
    "title": "Controlling for Log Age",
    "section": "",
    "text": "The issue here is that in regressions for some outcome - say firm valuation (the dreaded Q) - we want to look at the the change in the outcome variable around some treatment shock, but we want to control for some variables. The big ones here are firm age and firm size, using as a proxy firm market value (yes I realize it makes little sense to regress the firm valuation ratio — Tobin’s Q \\(\\approx\\) market value / book value — on the market value but it’s what people do).\nA problem arises when you use firm fixed effects and period fixed effects, because firm age becomes collinear with the fixed effect structure. As a result researchers will often control for the log of firm value instead, which is not perfectly collinear. This strikes me as very odd, because firm age is still an identity that comes directly from a firm variable (the first year in the panel) and a fixed effect (the period FE). So, I figured I would simulate some data to see if it works as we might hope it does.\nAssume we’re in a setting where the difference-in-differences (DiD) issues that I’ve written about previously here don’t apply - namely that there are many more non-treated units than treated units so that the negative weighting issue identified by Andrew Goodman-Bacon and others does not bite. Assume that we think the data generating process is as follows:\n\\[ y_{it} = \\alpha_i + \\alpha_t + \\delta T_{it} + \\epsilon_{it} \\\\\n\\alpha_i \\sim N(0, 1) \\\\\n\\alpha_t \\sim N(0, 1) \\\\\n\\epsilon_{it} \\sim N(0, 1) \\]\nThat is, our outcome variable for firm \\(i\\) in period \\(t\\) is a linear function of a firm specific effect \\(\\alpha_i\\), a period effect \\(\\alpha_t\\), a treatment effect \\(\\delta\\) and a stochastic error term \\(\\epsilon_{it}\\). Assume that, for some reason, we want to control for firm age, above and beyond the firm and period fixed effects.\nTo test this let’s create a reasonable simulation that approximates the types of regressions we do in empirical corporate finance work. Assume a panel of \\(N = 20,000\\) unique firms for which we have data on some over the period 1981-2010 (a thirty-year panel). Assume that firms enter at some point during that period (for each firm \\(i\\) I will randomly select a year in the period to be the firm founding year). In addition, not all firms make it to the end of the panel (they die out, or get bought, or reincorporate because they don’t enjoy paying taxes and being good corporate citizens), so their end year is randomly selected as some year after their first year but less than or equal to 2010. Finally, from the 20,000 firms, 4,000 are randomly selected to receive treatment in a randomly selected year between their first and last year in the panel. This is approximately what the panel structure looks like for most papers using some combination of Compustat and CRSP.\nFor the treatment effect structure, let’s assume that it manifests in a trend break. That is, the amount of treatment effect incurred by firm \\(i\\) in period \\(t\\), \\(\\delta_{it}\\), is equivalent to \\(0.2 \\cdot (year - treat\\_year + 1)\\), so the effect grows over time (it’s probably more realistic in these types of simulations to model dynamic treatment effects using some log function that grows over a period and then flatlines, but it doesn’t matter for analytical purposes.)\nFor our first analysis, let’s simulate data to make a panel with the structure described above (code is hidden below):\n\n\nCode\n# simulate data -----------------------------------------------------------\n\n# set seed \nset.seed(74792766)\n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data <- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  \n  # get a list of 4,000 units that are randomly treated sometime in the panel \n  treated_units <- sample(1:20000, 4000)\n  \n  # unit fixed effects\n  unit <- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1),\n    treat = if_else(unit %in% treated_units, 1, 0)) %>% \n    # make first and last year per unit, and treat year if treated\n    rowwise() %>% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year < 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010)),\n           # pull treat year as randomly selected year bw first and last if treated\n           treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0))) %>% \n    ungroup()\n  \n  # year fixed effects \n  year <- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  crossing(unit, year) %>% \n    arrange(unit, year) %>% \n    # keep only if year between first and last year \n    rowwise() %>% \n    filter(year %>% between(first_year, last_year)) %>% \n    ungroup() %>%\n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year >= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %>% \n    # make cumulative treatment effects\n    group_by(unit) %>% \n    mutate(cumtau = cumsum(tau)) %>% \n    ungroup() %>% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %>%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata <- make_data()\n\n\nLet’s assume we’re in a world where log of firm age variable (log_age, which is simply the log of year - first_year) has no effect on either the outcome variable or the treatment variable. Thus the causal relationship of the treatment on the outcome is simply reflected as:\n\n\nCode\ndagify(y ~ t, \n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\")) %>% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI’ll estimate the causal effect of the treatment on the outcome using a regular two-way fixed effects (TWFE) regression of the form:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it} + \\epsilon\\]\nThis is a normal event study design where we fully saturate in relative time indicators, but leave out two relative periods (following Borusyak & Jaravel (2017)), the most negative relative time indicator and the indicator for the year before treatment (t = -1). We know what the true treatment effect is here = 0.2 times the years since treatment for each post-treatment period. I will plot the true treatment effect, along with the estimates from regression (1) below:\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin <- min(data$rel_year, na.rm = TRUE) + 1\nmax <- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics <- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform <- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# get true taus to merge in\ntrue_taus <- tibble(\n  time = seq(-10, 10),\n  true_tau = c(rep(0, length(-10:-1)), .2*(0:10 + 1))\n)\n\n# estimate the model and make the plot\ndata %>% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %>% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %>%\n  filter(term != \"firm_age\") %>% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %>% \n  select(time, estimate, conf.low, conf.high) %>% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %>% \n  # keep just -10 to 10\n  filter(time %>% between(-10, 10)) %>% \n  left_join(true_taus) %>% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time < -1 ~ \"Pre\",\n    time >= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %>%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nLife is good, we recover the true treatment effect path. As an important aside, note that we fully saturated the time fixed effects here. What if we do what is more common in the literature and bin all year before t - 10 into a Pre variable and all years after t + 10 into a Post variable?\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin <- min(data$rel_year, na.rm = TRUE) + 1\nmax <- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics <- paste0(c(\"Pre\", \"Post\", \n                   paste0(\"`lag_\", seq(-10, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, 10))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform <- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %>% \n  # make pre and post variables\n  mutate(Pre = if_else(!is.na(rel_year) & rel_year < 10, 1, 0),\n         Post = if_else(!is.na(rel_year) & rel_year > 10, 1, 0)) %>%\n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %>% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %>%\n  filter(!(term %in% c(\"firm_age\", \"Pre\", \"Post\"))) %>%\n  # make the relative time variable and keep just what we need\n  mutate(time = c(-10:(-2), 0:10)) %>% \n  select(time, estimate, conf.low, conf.high) %>% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %>% \n  # keep just -10 to 10\n  filter(time %>% between(-10, 10)) %>% \n  left_join(true_taus) %>% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time < -1 ~ \"Pre\",\n    time >= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %>%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nWhoops! That’s no good. Try messing around with different forms of this, either only putting in the relative time indicators (thus leaving out both Pre and Post) or dropping one or the other, and you’ll see that things get really wonky. It depends on your DGP but we should all be fully saturating the relative time indicators every time.\nMoving on, what if you were to erroneously control for log age even though it isn’t related to either the outcome or the treatment? We’re still good (this should be obvious, but it’s nice to think through these relations and prove it to yourself).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# make string for rel time indicators \nindics <- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform <- as.formula(paste0(\"y ~ \", indics, \" + log_age | unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %>% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %>% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %>%\n  filter(term != \"log_age\") %>% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %>% \n  select(time, estimate, conf.low, conf.high) %>% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %>% \n  # keep just -10 to 10\n  filter(time %>% between(-10, 10)) %>% \n  left_join(true_taus) %>% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time < -1 ~ \"Pre\",\n    time >= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %>%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nOkay, let’s start to make things more interesting. First, what if our log_age variable is associated with our outcome variable, but not with the treatment. Now I change the DGP to be:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it}  - 0.85 \\cdot log\\_age + \\epsilon\\]\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %>% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nIf we run the regression controlling and control for log_age, we’re good. (Note, you shouldn’t need to control for log_age here because it isn’t a confounding variable - it only impacts the outcome variable. But, there are weird time dynamics that intersect with our relative period dummies and the omitted variables - essentially firm age is associated with the relative time dummies because time only goes in one direction and so does treatment assignment. Too tired to think through the consequences here, but with this DGP you do need to control for log_age to exactly hit the estimates with TWFE).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# next, assume that there is an effect on the outcome variable but no effect on the treatment outcome\n# make string for rel time indicators \nindics <- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\nform <- as.formula(paste0(\"y ~ \", indics, \"  + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %>% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %>% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %>% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %>% \n  mutate(time = c(min:(-2), 0:max)) %>% \n  select(time, estimate, conf.low, conf.high) %>% \n  # bring back in missing indicator for t = -1\n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %>% \n  # keep just -10 to 10\n  filter(time %>% between(-10, 10)) %>% \n  left_join(true_taus) %>% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time < -1 ~ \"Pre\",\n    time >= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %>%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16)) \n\n\n\n\n\n\n\n\n\nFinally, what if log_age is a confounding variable? That is, it impacts both the treatment and outcome variable and needs to be controlled for under causal-inference-dag-circle-drawing dogma:\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       t ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %>% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI implement this somewhat cludgely, but essentially I simulate the data in the same manner as above, but now the treatment assignment is in part determined by firm age. For each firm I get their average age in the panel and use that as an inverse weight when I randomly assign the treatment to firms (probably better ways to do this but it works). Now, not only is the estimate still valid when controlling for log_age but you in fact need to control for it to get unbiased estimates.\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# Now assume that the log age variable is correlated in some way with the treatment assignment decision\n# in particular assume that younger firms are more likely to get targeted \n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data <- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  # unit fixed effects\n  unit <- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1)) %>%\n    # make first and last year per unit, and treat year if treated\n    rowwise() %>% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year < 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010))) %>% \n    ungroup()\n  \n  # year fixed effects \n  year <- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  data <- crossing(unit, year) %>% \n    arrange(unit, year) %>% \n    # keep only if year between first and last year \n    rowwise() %>% \n    filter(year %>% between(first_year, last_year)) %>% \n    ungroup() %>% \n    mutate(firm_age = year - first_year)\n  \n    # make an average age data frame \n  avg_age <- data %>% \n    group_by(unit) %>% \n    summarize(avg_age = mean(firm_age)) %>% \n    mutate(weight = 1 / (avg_age + 1))\n  \n  # sample 4,000 firms to get treatment, weighted by average age\n  treated_units <- sample_n(avg_age, 4000, replace = FALSE, weight = weight) %>%\n    mutate(treat = 1) %>% \n    select(unit, treat)\n  \n  # merge treat back into the data \n  treat_data <- data %>% \n    select(unit, first_year, last_year) %>% \n    distinct() %>% \n    left_join(treated_units) %>% \n    replace_na(list(treat = 0)) %>% \n    rowwise() %>% \n    mutate(treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0)))\n  \n  # merge back into main data\n  data <- data %>% \n    left_join(treat_data) %>% \n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year >= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %>% \n    # make cumulative treatment effects\n    group_by(unit) %>% \n    mutate(cumtau = cumsum(tau)) %>% \n    ungroup() %>% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %>%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata2 <- make_data()\n\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin <- min(data2$rel_year, na.rm = TRUE) + 1\nmax <- max(data2$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics <- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform1 <- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\nform2 <- as.formula(paste0(\"y ~ \", indics, \" + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata2 %>% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %>% \n  # run the model and tidy up\n  do(bind_rows(\n    broom::tidy(felm(form1, data = .), conf.int = TRUE) %>% mutate(mod = \"No Control Log Age\"),\n    broom::tidy(felm(form2, data = .), conf.int = TRUE) %>% mutate(mod = \"Control Log Age\"))) %>% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %>% \n  mutate(time = rep(c(min:(-2), 0:max), 2)) %>% \n  select(time, estimate, conf.low, conf.high, mod) %>% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = rep(-1, 2), estimate = rep(0, 2), conf.low = rep(0, 2),\n    conf.high = rep(0, 2), mod = c(\"No Control Log Age\", \"Control Log Age\")\n  )) %>% \n  # keep just -10 to 10\n  filter(time %>% between(-10, 10)) %>% \n  left_join(true_taus) %>% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time < -1 ~ \"Pre\",\n    time >= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %>%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16),\n        panel.spacing = unit(2, \"lines\")) + \n  facet_wrap(~mod)\n\n\n\n\n\n\n\n\n\nIn conclusion, at least in this very straightforward example, you do need to control for the log of firm age, even though firm age itself is collinear with the fixed effects structure. Mea culpa."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Blogpost documenting the sensitivity of estimates of the effect of medical marijuana legalization on opioid mortality.\n\n\n\n\n\n\n06-2020\n\n\n\n\n\n\n\n\n\n\n\n\nShort blogpost summarizing how and why one would control for the log of the age of a firm\n\n\n\n\n\n\n06-2020\n\n\n\n\n\n\n\n\n\n\n\n\nShort blogpost summarizing key attributes of the new difference-in-differences literature\n\n\n\n\n\n\n09-2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "7b5f762d/research/working_papers/antitakeover/index.html",
    "href": "7b5f762d/research/working_papers/antitakeover/index.html",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "",
    "text": "Paper\nSSRN\nCode"
  },
  {
    "objectID": "7b5f762d/research/working_papers/antitakeover/index.html#abstract",
    "href": "7b5f762d/research/working_papers/antitakeover/index.html#abstract",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Abstract",
    "text": "Abstract\nCorporate governance scholars have engaged in a longstanding debate over the impact of state antitakeover provisions. Corporate law practitioners and researchers argue that the affirmation of the “poison pill” made the second generation of antitakeover statutes redundant, while empirical scholars in corporate finance continue to find wide-ranging impacts from their adoption. This paper subjects the standard approach used in the empirical literature to a series of straightforward extensions using current best practice in panel data analysis. Contrary to the majority of published research, there is scant evidence for a consistent and reliable impact of antitakeover statute adoption on firm activity. These findings follow logically from the legal argument that takeover statutes provide little additional takeover deterrence in the presence of a “shadow pill.”"
  },
  {
    "objectID": "7b5f762d/research/working_papers/antitakeover/index.html#important-figure",
    "href": "7b5f762d/research/working_papers/antitakeover/index.html#important-figure",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Important figure",
    "text": "Important figure\nFigure 10 reports the event study estimates of the impact of poison pill law changes using the estimator from Callaway and Sant’Anna (2020) and the data and design changes described in Section 5.1. Model 1 includes only the fixed effects without any covariates, Model 2 includes the covariates in the short regression model from Karpoff and Wittry (2018), and Model 3 includes their full model.\n\n\n\nFigure 10"
  },
  {
    "objectID": "7b5f762d/research/articles/slr/index.html",
    "href": "7b5f762d/research/articles/slr/index.html",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "",
    "text": "Paper\nCode\nData"
  },
  {
    "objectID": "7b5f762d/research/articles/slr/index.html#abstract",
    "href": "7b5f762d/research/articles/slr/index.html#abstract",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Abstract",
    "text": "Abstract\nLawsuits brought pursuant to section 10(b) of the Securities and Exchange Act depend on the reliability of a statistical tool called an event study to adjudicate issues of reliance, materiality, loss causation, and damages. Although judicial acceptance of the event study technique is pervasive, there has been little empirical analysis of the ability of event studies to produce reliable results when applied to a single company’s security.\nUsing data from the recent financial crisis, this Note demonstrates that the standard-model event study used in most court proceedings can lead to biased inferences sanctioned through the Daubert standard of admissibility for expert testimony. In particular, in the presence of broad market volatility, a base event study will cause too many returns to be identified as statistically significant. Even recently proposed variations of the event study model specifically designed to address violations of the statistical assumptions of an event study will not completely correct this bias.\nThis Note proposes two alternative forms of event studies that are capable of creating statistically reliable results and should be adopted by courts in instances where there is cause to believe that market volatility has increased. Over previous decades, the judiciary has steadily moved toward a reliance on empirics and expert testimony in overseeing complex civil cases. Yet there has been surprisingly little research accompanying this judicial deference on the ability of statistical evidence to produce the promised result. This Note calls into question whether this movement has been beneficial from a logical or empirical perspective, but it demonstrates that alternative techniques that can aid the finder of fact in resolving these disputes—regardless of market trends—may in fact exist."
  },
  {
    "objectID": "7b5f762d/research/articles/slr/index.html#important-figure",
    "href": "7b5f762d/research/articles/slr/index.html#important-figure",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Important figure",
    "text": "Important figure\nThe top panel of Fig. 3 illustrates the diagnostic test for Simulations 4, 5, and 6. Because the diagnostic test only applies to balanced panels, in constructing this figure our simulation is modified to artificially induce a balanced panel of firm-year observations from Compustat before drawing fixed effects and residuals from the empirical distribution.\n\n\n\nFigure 3\n\n\n@article{baker2016single,\n  title={Single-firm event studies, securities fraud, and financial crisis: problems of inference},\n  author={Baker, Andrew C},\n  journal={Stan. L. Rev.},\n  volume={68},\n  pages={1207},\n  year={2016},\n  publisher={HeinOnline}\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew C. Baker",
    "section": "",
    "text": "I am an Assistant Professor at Berkeley Law School, and a recent PhD and law graduate from Stanford University. My research interests include corporate governance, empirical legal studies, and law and economics/finance."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew C. Baker",
    "section": "Education",
    "text": "Education\nStanford University | Stanford, CA\nPhD in Business Administration | 2021\nStanford University | Stanford, CA\nJuris Doctor | 2017\nGeorgetown University| Washington, DC\nBSc in International Economics | 2009"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]